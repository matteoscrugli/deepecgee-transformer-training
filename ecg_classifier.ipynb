{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4435,"status":"ok","timestamp":1684271494282,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"_Yg_guwuP-aY","outputId":"9e8ebc3c-efe2-499b-b40f-9195def5d764"},"outputs":[],"source":["import keras\n","print(keras.__version__)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1684271494283,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"3HuzLgSY6rUi","outputId":"95eff1b5-63f1-4f88-9343-751c0961a63f"},"outputs":[],"source":["try:\n","  import google.colab\n","  IN_COLAB = True\n","  print('colab_env')\n","except:\n","  IN_COLAB = False\n","  print('local_env')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20788,"status":"ok","timestamp":1684271515057,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"o0pL5xCvsfcU","outputId":"11adaad8-c3e5-4b9a-ff24-42940d3e0775"},"outputs":[],"source":["if IN_COLAB:\n","  from google.colab import drive\n","  drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14130,"status":"ok","timestamp":1684271529173,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"kJ5Sky3dsmCX","outputId":"c1cec8cc-2d3b-4847-91e9-58e0df49d1d0"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import json\n","import pickle\n","import tensorflow as tf\n","import os\n","from os import path\n","import argparse\n","import shutil\n","from itertools import repeat\n","from tensorflow.keras import layers\n","from keras.callbacks import ModelCheckpoint\n","from sklearn.utils import class_weight\n","from pathlib import Path\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n","import seaborn as sns\n","import random\n","from tensorflow import keras\n","from keras.models import load_model\n","import keras\n","import scipy.signal\n","import re\n","\n","if IN_COLAB:\n","    !pip install wfdb\n","import wfdb"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1684271529174,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"HcCKVRK1spP1"},"outputs":[],"source":["# Load the ECG signal and annotations from a JSON file\n","def load_ecg_data_from_json(file_path):\n","    with open(file_path, 'r') as infile:\n","        data = json.load(infile)\n","    signal = np.array(data['signal'])\n","    annotations = wfdb.Annotation(record_name=data['record_name'],\n","                                   extension=data['extension'],\n","                                   sample=data['sample'],\n","                                   symbol=data['symbol'],\n","                                   subtype=data['subtype'],\n","                                   chan=data['chan'],\n","                                   num=data['num'],\n","                                   aux_note=data['aux_note'])\n","    return signal, annotations\n","\n","def load_ECG_MIT_BIH_mixedtestset_7_1_2_mod(half_window, overwrite, test, split, iswfdbdataset, dataset_path): # , session_path\n","\n","  #dataset_path = \"/media/paola/T7/Datasets/ECG_MITBIH/mit-bih-arrhythmia-database-1.0.0/\"\n","  random_seed = 0# 60 #0 \n","  dataset_split = 0.7\n","  normalization = 0\n","  denoising = 1\n","  not_centered = 0\n","  base = 0\n","\n","  augmentation = [0,1]#[16, 3]\n","  dataset_name = \"NSVFQ\"\n","  print(\"DATASET NAME IS \",dataset_name)\n","  data_names = ['100', '101', '103', '105', '106', #'107',\n","              '108', '109', '111', '112', '113', '114', '115', '116',\n","              '117', '118', '119', '121', '122', '123', '124', '200',\n","              '201', '202', '203', '205', '207', '208', '209', '210',\n","              '212', '213', '214', '215', '219', '220', '221', #'217', \n","              '222', '223', '228', '230', '231', '232', '233', '234']\n","\n","  test_set = []\n","  while 0:#(len(test_set)<5):\n","    test_record = random.choice(data_names)\n","    if test_record not in test_set:\n","      test_set.append(test_record)\n","  print(\"Test set is\",test_set)\n","  test_record = 0\n","  a = 0\n","  b = 0\n","#############################################################################################################\n","#Create session\n","  temp = 's'\n","  #for s in subjects:\n","  #    temp += f'_{s}'\n","  #session_path = f\"/content/gdrive/MyDrive/trained_models/model6_7_1_2_balanced/\"\n","  #session_path = \"output/train_raw/\" + session_name + \"/\"\n","\n","  # if test == 0:\n","  #   if os.path.isdir(session_path):\n","  #     if overwrite:\n","  #       try:\n","  #           shutil.rmtree(session_path)\n","  #           Path(session_path).mkdir(parents=True, exist_ok=True)\n","  #       except OSError:\n","  #           print(\"Error in session creation (\"+session_path+\").\")\n","  #           exit()\n","  #     else:\n","  #       print(f'Session path ({session_path}) already exists')\n","  #       exit()\n","  #   else:\n","  #     try:\n","  #       Path(session_path).mkdir(parents=True, exist_ok=True)\n","  #     except OSError:\n","  #       print(\"Error in session creation (\"+session_path+\").\")\n","  #       exit()\n","#############################################################################################################\n","\n","  if dataset_name == 'NLRAV':\n","    labels = ['N', 'L', 'R', 'A', 'V']\n","    sub_labels = {'N':'N', 'L':'L', 'R':'R', 'A':'A', 'V':'V'}\n","\n","  elif dataset_name == 'NSVFQ':\n","    labels = ['N', 'S', 'V', 'F', 'Q']\n","    sub_labels = { 'N':'N', 'L':'N', 'R':'N', 'e':'N', 'j':'N',\n","                   'A':'S', 'a':'S', 'J':'S', 'S':'S',\n","                   'V':'V', 'E':'V',\n","                   'F':'F',\n","                   '/':'Q', 'f':'Q', 'Q':'Q'}\n","\n","  X = []\n","  Y = []\n","  C = []\n","  R = []\n","  RR = []\n","  # f_log = open(session_path + \"dataload_log.txt\", \"w\") \n","  # f_log.write(\"Dataset \" + dataset_name + \"\\n\")\n","\n","\n","  if denoising == 1:\n","    denoise_fir = scipy.signal.firwin(13, 35, width=None, window='hamming', pass_zero='lowpass', scale=True, nyq=None, fs=360)\n","    print(denoise_fir)\n","    base = 6\n","  if not_centered == 1:\n","    with open('matrix_l_mod.pickle', 'rb') as input_file:\n","      matrix_l_mod = pk.load(input_file)\n","\n","  for d in data_names:\n","    count_labels = np.zeros(5)\n","\n","    # r = wfdb.rdrecord(dataset_path+d)\n","    # ann = wfdb.rdann(dataset_path+d, 'atr', return_label_elements=['label_store', 'symbol'])\n","    # if d!='114':\n","    #     sig = np.array(r.p_signal[:,0])\n","    #     intsig = np.array(r.p_signal[:,0])\n","    # else:\n","    #     sig = np.array(r.p_signal[:,1])\n","    #     intsig = np.array(r.p_signal[:,1])\n","\n","    if iswfdbdataset:\n","      r = wfdb.rdrecord(dataset_path+d)\n","      ann = wfdb.rdann(dataset_path+d, 'atr', return_label_elements=['label_store', 'symbol'])\n","      if d!='114':\n","          sig = np.array(r.p_signal[:,0])\n","          intsig = np.array(r.p_signal[:,0])\n","      else:\n","          sig = np.array(r.p_signal[:,1])\n","          intsig = np.array(r.p_signal[:,1])\n","    else:\n","      sig, ann = load_ecg_data_from_json(dataset_path + d + '.json')\n","      if d != '114':\n","          sig = sig[:, 0]\n","      else:\n","          sig = sig[:, 1]\n","\n","    if denoising == 1:\n","      basewond_sig = scipy.signal.medfilt(sig,71)\n","      basewond_sig2 = scipy.signal.medfilt(basewond_sig,215)\n","      sig = sig - basewond_sig2\n","      denoise_sig = scipy.signal.convolve(sig,denoise_fir)\n","    sig_len = len(sig)\n","    sym = ann.symbol\n","    pos = ann.sample\n","    if not_centered == 1:\n","      detected_pos = []\n","      label_pos = []\n","      file_index = data_names.index(d)\n","      matrix_line = matrix_l_mod[file_index]\n","      for element in matrix_line:\n","        if len(element) == 1:\n","          detected_pos.append(element[0][1])\n","          if detected_pos[-1] is not None:\n","            detected_pos[-1] = detected_pos[-1] + base\n","          label_pos.append(element[0][0] + base)\n","        else:\n","          print(\"Double peak detected, choosing first\")\n","          detected_pos.append(element[0][1])\n","          if detected_pos[-1] is not None:\n","            detected_pos[-1] = detected_pos[-1] + base\n","          label_pos.append(element[0][0] + base)\n","          print(detected_pos[-1])\n","      detected_pos = np.array(detected_pos)\n","    if denoising == 1:\n","     for i in range(len(pos)):\n","       pos[i] = pos[i] + base #adjust delay connected to powerline denoising with 12-order FIR filter\n","     sig = denoise_sig\n","    beat_len = len(sym)\n","    pre = None\n","    detected_index = 0\n","    #base =  0\n","    if d not in test_set:\n","      print(\"Appending data in \" + d + \" to train data\")\n","      # f_log.write(\"Appending data in \" + d + \" to train data\\n\")\n","      for i in range(beat_len):\n","        for j in range(-augmentation[0]*augmentation[1],augmentation[0]*augmentation[1]+1,augmentation[1]):\n","            if pos[i]-half_window+j>=base and pos[i]+half_window+j<=(sig_len+base) and sym[i] in sub_labels:\n","                if not_centered == 1:\n","                    if detected_pos[detected_index] is not None:\n","                      if detected_pos[detected_index]+half_window+j > (sig_len+base):\n","                        frame = sig[detected_pos[detected_index]-half_window+j:sig_len+base]\n","                      elif detected_pos[detected_index]-half_window+j < base:\n","                        frame = sig[base:detected_pos[detected_index]+half_window+j]\n","                      else:\n","                        frame = sig[detected_pos[detected_index]-half_window+j:detected_pos[detected_index]+half_window+j]\n","                      print(d, detected_pos[detected_index], pos[i])\n","#                      print(detected_pos[detected_index]-half_window,detected_pos[detected_index]+half_window+j, len(frame))\n","                      detected_index = detected_index + 1\n","                    else:\n","                      frame = np.zeros(198)\n","                      detected_index = detected_index + 1\n","                    X.append(frame)\n","               #     print(a)\n","                    a = a + 1\n","                    if detected_pos[detected_index-1] is None or len(frame)<198:\n","                      C.append(False)   # if the peak is not detected remove from testset after split\n","                      X[-1] = sig[pos[i]-half_window+j:pos[i]+half_window+j]\n","                    elif pre is not None:\n","                      if pos[i]-pre<half_window:\n","                        print(\"ATTENTION, OVERLAPPING WINDOWS\")\n","                        print(pre,pos[i])\n","                      C.append(True if (j == 0 and (pos[i]-pre)>half_window)  else False)\n","                    else:\n","                      C.append(True if (j == 0) else False)\n","                    R.append([data_names.index(d),pos[i]-base, detected_pos[detected_index-1]])\n","                    post = 0\n","                    index = i\n","                    # eval postRR\n","                    if detected_index < len(detected_pos):\n","                      post = detected_pos[detected_index] if detected_pos[detected_index] is not None else None\n","                    else:\n","                      post = None \n","                    if post is None or detected_pos[detected_index-1] is None: # if next peek or current peak is not detected set to maximum distance\n","                      postRR = 500\n","                    elif post-detected_pos[detected_index-1] > 500:\n","                      postRR = 500\n","                    else:\n","                      postRR = post-detected_pos[detected_index-1]\n","                    # eval preRR\n","                    if detected_index >=2:\n","                      pre = detected_pos[detected_index-2]\n","                    else:\n","                      pre = base # for the first pek consider the start of the record\n","                    if pre is None or detected_pos[detected_index - 1] is None: # if previous peek or current peak is not detected set to maximum distance\n","                      preRR = 500\n","                    elif detected_pos[detected_index-1]-pre > 500:\n","                      preRR = 500\n","                    else:\n","                     preRR = detected_pos[detected_index-1]-pre\n","                elif not_centered == 0:\n","                  frame = sig[pos[i]-half_window+j:pos[i]+half_window+j]\n","                  X.append(frame)\n","                #  print(a)\n","                  a = a + 1\n","                  if pre is not None:\n","                   if pos[i]-pre<half_window:\n","                     print(\"ATTENTION, OVERLAPPING WINDOWS\")\n","                     print(pre,pos[i])\n","                   C.append(True if (j == 0 and (pos[i]-pre)>half_window)  else False)\n","                  else:\n","                   C.append(True if (j == 0) else False)\n","                  R.append([data_names.index(d),pos[i]])\n","                  post = 0\n","                  index = i\n","                  while post == 0 and index<(beat_len-1):\n","                    if sym[index+1] in sub_labels:\n","                      post = pos[index+1]\n","                    else:\n","                      index = index + 1\n","                  if pre is None:\n","                      preRR = pos[i]\n","                  elif pos[i]-pre > 500:\n","                      preRR = 500\n","                  else:\n","                     preRR = pos[i]-pre\n","                  if post-pos[i] > 500:\n","                     postRR = 500\n","                  else:\n","                     postRR = post-pos[i]\n","                Y.append(labels.index(sub_labels[sym[i]]))\n","                count_labels[labels.index(sub_labels[sym[i]])] = count_labels[labels.index(sub_labels[sym[i]])] + 1\n","                RR.append([preRR,postRR])\n","                pre = pos[i]\n","            elif not_centered == 1 and pos[i] == label_pos[detected_index] :  # skip the first detected peak if needed to match the first admissible window based on label peaks\n","              detected_index = detected_index +  1\n","    print(\"Patient \" + d + \" includes labels \" + str(int(count_labels[0])) + \" \" + str(int(count_labels[1])) + \" \" + str(int(count_labels[2])) + \" \" + str(int(count_labels[3])) + \" \" + str(int(count_labels[4])) )\n","    # f_log.write(\"Patient \" + d + \" includes labels \" + str(int(count_labels[0])) + \" \" + str(int(count_labels[1])) + \" \" + str(int(count_labels[2])) + \" \" + str(int(count_labels[3])) + \" \" + str(int(count_labels[4])))\n","    # f_log.write(\"\\n\")\n","  item_perm = np.arange(np.size(X,0))\n","  np.random.seed(random_seed)\n","  np.random.shuffle(item_perm)\n","\n","#  f = open(session_path + \"R_all.txt\",\"w\")\n"," # for i in np.array(R):\n"," #   s = str(i[0]) + \" \" + str(i[1]) + \"\\n\"#\" \" + str(i[2]) + \"\\n\"\n"," #   f.write(s)\n"," # f.close()\n","  print(np.array(X).shape)\n","\n","  X = np.array(X)[item_perm]\n","  Y = np.array(Y)[item_perm]\n","  C = np.array(C)[item_perm]\n","  R = np.array(R)[item_perm]\n","  RR = np.array(RR)[item_perm]\n","\n","  print(split)\n","  if split == 1:\n","    X_train = X[:round(np.size(X,0)*dataset_split)]\n","    Y_train = Y[:round(np.size(X,0)*dataset_split)]\n","    RR_train = RR[:round(np.size(X,0)*dataset_split)]\n","  # C_train = C[:round(np.size(X,0)*dataset_split)]\n","    R_train = R[:round(np.size(X,0)*dataset_split)]\n","  # P_train = P[:round(np.size(X,0)*dataset_split)]\n","\n","    X_valid_ = X[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)] \n","    Y_valid_ = Y[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)]\n","    C_valid = C[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)]\n","    R_valid = R[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)]\n","    RR_valid = RR[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)]\n","                                                        \n","    X_test_ = X[round(np.size(X,0)*0.8):] \n","    Y_test_ = Y[round(np.size(X,0)*0.8):]\n","    C_test = C[round(np.size(X,0)*0.8):]\n","    R_test = R[round(np.size(X,0)*0.8):]\n","    RR_test = RR[round(np.size(X,0)*0.8):]\n","\n","  elif split == 2:\n","    X_test_ = X[:round(np.size(X,0)*0.2)] \n","    Y_test_ = Y[:round(np.size(X,0)*0.2)]\n","    C_test = C[:round(np.size(X,0)*0.2)]\n","    R_test = R[:round(np.size(X,0)*0.2)]\n","    RR_test = RR[:round(np.size(X,0)*0.2)]\n","\n","    X_valid_ = X[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)] \n","    Y_valid_ = Y[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)]\n","    C_valid = C[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)]\n","    R_valid = R[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)]\n","    RR_valid = RR[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)]\n","\n","    X_train = X[round(np.size(X,0)*0.3):]\n","    Y_train = Y[round(np.size(X,0)*0.3):]\n","    RR_train = RR[round(np.size(X,0)*0.3):]\n","  # C_train = C[:round(np.size(X,0)*dataset_split)]\n","    R_train = R[round(np.size(X,0)*0.3):]\n","  # P_train = P[:round(np.size(X,0)*dataset_split)]\n","\n","  elif split == 3:\n","    X_test_ = X[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)] \n","    Y_test_ = Y[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)]\n","    C_test = C[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)]\n","    R_test = R[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)]\n","    RR_test = RR[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)]\n","\n","    X_valid_ = X[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)] \n","    Y_valid_ = Y[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)]\n","    C_valid = C[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)]\n","    R_valid = R[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)]\n","    RR_valid = RR[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)]\n","\n","    X_train = X[:round(np.size(X,0)*0.2)]\n","    Y_train = Y[:round(np.size(X,0)*0.2)]\n","    RR_train = RR[:round(np.size(X,0)*0.2)]\n","    R_train = R[:round(np.size(X,0)*0.2)]\n","\n","    X_train = np.append(X_train, X[round(np.size(X,0)*0.5):], axis = 0)\n","    Y_train = np.append(Y_train, Y[round(np.size(X,0)*0.5):], axis = 0)\n","    RR_train = np.append(RR_train, RR[round(np.size(X,0)*0.5):], axis = 0)\n","    R_train = np.append(R_train, R[round(np.size(X,0)*0.5):], axis = 0)\n","\n","  elif split == 4:\n","    X_test_ = X[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)] \n","    Y_test_ = Y[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)]\n","    C_test = C[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)]\n","    R_test = R[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)]\n","    RR_test = RR[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)]\n","\n","    X_valid_ = X[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)] \n","    Y_valid_ = Y[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)]\n","    C_valid = C[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)]\n","    R_valid = R[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)]\n","    RR_valid = RR[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)]\n","\n","    X_train = X[:round(np.size(X,0)*0.4)] \n","    Y_train = Y[:round(np.size(X,0)*0.4)]\n","    RR_train = RR[:round(np.size(X,0)*0.4)]\n","    R_train = R[:round(np.size(X,0)*0.4)]\n","\n","    X_train = np.append(X_train, X[round(np.size(X,0)*0.7):], axis = 0)\n","    Y_train = np.append(Y_train, Y[round(np.size(X,0)*0.7):], axis = 0)\n","    RR_train = np.append(RR_train, RR[round(np.size(X,0)*0.7):], axis = 0)\n","    R_train = np.append(R_train, R[round(np.size(X,0)*0.7):], axis = 0)\n","\n","  elif split == 5:\n","    X_test_ = X[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)] \n","    Y_test_ = Y[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)]\n","    C_test = C[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)]\n","    R_test = R[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)]\n","    RR_test = RR[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)]\n","\n","    X_valid_ = X[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)] \n","    Y_valid_ = Y[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)]\n","    C_valid = C[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)]\n","    R_valid = R[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)]\n","    RR_valid = RR[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)]\n","\n","    X_train = X[:round(np.size(X,0)*0.6)]\n","    Y_train = Y[:round(np.size(X,0)*0.6)]\n","    RR_train = RR[:round(np.size(X,0)*0.6)]\n","    R_train = R[:round(np.size(X,0)*0.6)]\n","\n","    X_train = np.append(X_train, X[round(np.size(X,0)*0.9):], axis = 0)\n","    Y_train = np.append(Y_train, Y[round(np.size(X,0)*0.9):], axis = 0)\n","    RR_train = np.append(RR_train, RR[round(np.size(X,0)*0.9):], axis = 0)\n","    R_train = np.append(R_train, R[round(np.size(X,0)*0.9):], axis = 0)\n","\n","  X_valid = X_valid_[C_valid]\n","  Y_valid = Y_valid_[C_valid]\n","  RR_valid = RR_valid[C_valid]\n","  X_test = X_test_[C_test]\n","  Y_test = Y_test_[C_test]\n","  RR_test = RR_test[C_test]\n","  R_valid = R_valid[C_valid]\n","  R_test = R_test[C_test]\n"," # f = open(session_path + \"R_detected.txt\",\"w\")\n"," # for i in R_test:\n"," #   s = str(i[0]) + \" \" + str(i[1]) + \" \" + str(i[2]) +\"\\n\"\n"," #   f.write(s)\n"," # f.close()\n","\n","  print(\"Validation set\",X_valid.shape)\n","  print(\"Test set\",X_test.shape)\n","  if normalization:\n","    for i in range(np.size(X_train,0)):\n","        X_train[i]=X_train[i]/np.max(np.absolute(X_train[i]))\n","    for i in range(np.size(X_valid,0)):\n","        X_valid[i]=X_valid[i]/np.max(np.absolute(X_valid[i]))\n","    for i in range(np.size(X_test,0)):\n","        X_test[i]=X_test[i]/np.max(np.absolute(X_test[i]))\n","\n","  X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1,1)\n","  X_valid = X_valid.reshape(X_valid.shape[0],X_valid.shape[1],1,1)\n","  X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1,1)\n","\n","  print(\"X train_shape\",X_train.shape)\n","  print(\"X valid_shape\",X_valid.shape)\n","  print(\"Y train shape\", Y_train.shape)\n","  print(\"Y valid shape\", Y_valid.shape)\n","  print(\"X_test shape\", X_test.shape)\n","  print(\"Y test shape\", Y_test.shape)\n","\n","  print(\"RR_train\", RR_train.shape)\n","  print(\"RR_valid\", RR_valid.shape)\n","  print(\"RR_test\", RR_test.shape)\n","\n","  class_types = labels\n","  # f_log.close()\n","\n","  return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, R_train, R_valid, R_test, RR_train, RR_valid, RR_test, class_types\n","\n","\n","def load_ECG_MIT_BIH_mixedtestset_7_1_2(half_window, session_name, overwrite, test, split, in_colab):\n","\n","  wfdbdataset = False\n","\n","  if wfdbdataset:\n","    if in_colab:\n","      dataset_path = \"/content/gdrive/MyDrive/dataset/mit-bih-arrhythmia-database-1.0.0/\"\n","    else:\n","      dataset_path = \"../../dataset/mit-bih-arrhythmia-database-1.0.0/\"\n","  else:\n","    if in_colab:\n","      dataset_path = \"/content/gdrive/MyDrive/dataset/mit-bih-arrhythmia-database-1.0.0_em24/\"\n","    else:\n","      dataset_path = \"../../dataset/mit-bih-arrhythmia-database-1.0.0_em24/\"\n","\n","  random_seed = 0# 60 #0 \n","  dataset_split = 0.7\n","  normalization = 0\n","  denoising = 1\n","  base = 0\n","\n","  augmentation = [0,1]#[16, 3]\n","  dataset_name = \"NSVFQ\"\n","  print(\"DATASET NAME IS \",dataset_name)\n","  data_names = ['100', '101', '103', '105', '106', #'107',\n","              '108', '109', '111', '112', '113', '114', '115', '116',\n","              '117', '118', '119', '121', '122', '123', '124', '200',\n","              '201', '202', '203', '205', '207', '208', '209', '210',\n","              '212', '213', '214', '215', '219', '220', '221', #'217', \n","              '222', '223', '228', '230', '231', '232', '233', '234']\n","\n","  test_set = []\n","  while 0:#(len(test_set)<5):\n","    test_record = random.choice(data_names)\n","    if test_record not in test_set:\n","      test_set.append(test_record)\n","  print(\"Test set is\",test_set)\n","  test_record = 0\n","\n","\n","#############################################################################################################\n","#Create session\n","  if in_colab:\n","    session_path = f\"/content/gdrive/MyDrive/output/train/em24/\"\n","  else:\n","    session_path = f\"output/train/test_noise/\"\n","\n","  os.makedirs(session_path, exist_ok=True)\n"," \n","#############################################################################################################\n","\n","\n","  if dataset_name == 'NLRAV':\n","    labels = ['N', 'L', 'R', 'A', 'V']\n","    sub_labels = {'N':'N', 'L':'L', 'R':'R', 'A':'A', 'V':'V'}\n","\n","  elif dataset_name == 'NSVFQ':\n","    labels = ['N', 'S', 'V', 'F', 'Q']\n","    sub_labels = { 'N':'N', 'L':'N', 'R':'N', 'e':'N', 'j':'N',\n","                   'A':'S', 'a':'S', 'J':'S', 'S':'S',\n","                   'V':'V', 'E':'V',\n","                   'F':'F',\n","                   '/':'Q', 'f':'Q', 'Q':'Q'}\n","\n","  X = []\n","  Y = []\n","  C = []\n","  R = []\n","  RR = []\n","  f_log = open(session_path + \"dataload_log.txt\", \"w\") \n","  f_log.write(\"Dataset \" + dataset_name + \"\\n\")\n","\n","  if denoising == 1:\n","    denoise_fir = scipy.signal.firwin(13, 35, width=None, window='hamming', pass_zero='lowpass', scale=True, nyq=None, fs=360)\n","    print(denoise_fir)\n","    base = 6\n","\n","  for d in data_names:\n","    count_labels = np.zeros(5)\n","    if wfdbdataset:\n","      r = wfdb.rdrecord(dataset_path+d)\n","      ann = wfdb.rdann(dataset_path+d, 'atr', return_label_elements=['label_store', 'symbol'])\n","      if d!='114':\n","          sig = np.array(r.p_signal[:,0])\n","          intsig = np.array(r.p_signal[:,0])\n","      else:\n","          sig = np.array(r.p_signal[:,1])\n","          intsig = np.array(r.p_signal[:,1])\n","    else:\n","      sig, ann = load_ecg_data_from_json(dataset_path + d + '.json')\n","      if d != '114':\n","          sig = sig[:, 0]\n","      else:\n","          sig = sig[:, 1]\n","    if denoising == 1:\n","      print(sig.shape)\n","      plt.plot(sig[:4000],label=\"original\")\n","      basewond_sig = scipy.signal.medfilt(sig,71)\n","      basewond_sig2 = scipy.signal.medfilt(basewond_sig,215)\n","      #sig = sig - basewond_sig\n","      sig = sig - basewond_sig2\n","      denoise_sig = scipy.signal.convolve(sig,denoise_fir)\n","      plt.plot(denoise_sig[6:4006], label=\"denoised\")\n","      plt.legend()\n","      plt.show()\n","    sig_len = len(sig)\n","    sym = ann.symbol\n","    pos = ann.sample\n","    beat_len = len(sym)\n","    if denoising == 1:\n","      for i in range(len(pos)):\n","         pos[i] = pos[i] + base #adjust delay connected to powerline denoising with 12-order FIR filter\n","      sig = denoise_sig\n","    pre = None\n","    if d not in test_set:\n","      print(\"Appending data in \" + d + \" to train data\")\n","      f_log.write(\"Appending data in \" + d + \" to train data\\n\")\n","      for i in range(beat_len):\n","        for j in range(-augmentation[0]*augmentation[1],augmentation[0]*augmentation[1]+1,augmentation[1]):\n","            if pos[i]-half_window+j>=base and pos[i]+half_window+j<=(sig_len+base) and sym[i] in sub_labels:\n","                frame = sig[pos[i]-half_window+j:pos[i]+half_window+j]\n","                X.append(frame)\n","                Y.append(labels.index(sub_labels[sym[i]]))\n","                count_labels[labels.index(sub_labels[sym[i]])] = count_labels[labels.index(sub_labels[sym[i]])] + 1\n","                if pre is not None:\n","                   if pos[i]-pre<half_window:\n","                     print(\"ATTENTION, OVERLAPPING WINDOWS\")\n","                     print(pre,pos[i])\n","                   C.append(True if (j == 0 and (pos[i]-pre)>half_window) else False)\n","                else:\n","                   C.append(True if (j == 0) else False)\n","                R.append(data_names.index(d))\n","                post = 0\n","                index = i\n","                while post == 0 and index<(beat_len-1):\n","                  if sym[index+1] in sub_labels:\n","                    post = pos[index+1]\n","                  else:\n","                    index = index + 1\n","                if pre is None:\n","                   preRR = pos[i]\n","                elif pos[i]-pre > 500:\n","                   preRR = 500\n","                  # print(\"More than 500 Pre\")\n","                else:\n","                   preRR = pos[i]-pre\n","                if post-pos[i] > 500:\n","                   postRR = 500\n","                 #  print(\"More than 500 Post\")\n","                else:\n","                   postRR = post-pos[i]\n","                RR.append([preRR,postRR])\n","                pre = pos[i]\n","    print(\"Patient \" + d + \" includes labels \" + str(int(count_labels[0])) + \" \" + str(int(count_labels[1])) + \" \" + str(int(count_labels[2])) + \" \" + str(int(count_labels[3])) + \" \" + str(int(count_labels[4])) )\n","    f_log.write(\"Patient \" + d + \" includes labels \" + str(int(count_labels[0])) + \" \" + str(int(count_labels[1])) + \" \" + str(int(count_labels[2])) + \" \" + str(int(count_labels[3])) + \" \" + str(int(count_labels[4])))\n","    f_log.write(\"\\n\")\n","  item_perm = np.arange(np.size(X,0))\n","\n","  np.random.seed(random_seed)\n","  np.random.shuffle(item_perm)\n","\n","  X = np.array(X)[item_perm]\n","  Y = np.array(Y)[item_perm]\n","  C = np.array(C)[item_perm]\n","  R = np.array(R)[item_perm]\n","  RR = np.array(RR)[item_perm]\n","\n","  if split == 1:\n","    print(\"Split 1\")\n","    X_train = X[:round(np.size(X,0)*dataset_split)]\n","    Y_train = Y[:round(np.size(X,0)*dataset_split)]\n","    RR_train = RR[:round(np.size(X,0)*dataset_split)]\n","  # C_train = C[:round(np.size(X,0)*dataset_split)]\n","    R_train = R[:round(np.size(X,0)*dataset_split)]\n","  # P_train = P[:round(np.size(X,0)*dataset_split)]\n","\n","    X_valid_ = X[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)] \n","    Y_valid_ = Y[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)]\n","    C_valid = C[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)]\n","    R_valid = R[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)]\n","    RR_valid = RR[round(np.size(X,0)*dataset_split):round(np.size(X,0)*0.8)]\n","                                                        \n","    X_test_ = X[round(np.size(X,0)*0.8):] \n","    Y_test_ = Y[round(np.size(X,0)*0.8):]\n","    C_test = C[round(np.size(X,0)*0.8):]\n","    R_test = R[round(np.size(X,0)*0.8):]\n","    RR_test = RR[round(np.size(X,0)*0.8):]\n","\n","  elif split == 2:\n","    print(\"Split 2\")\n","    X_test_ = X[:round(np.size(X,0)*0.2)] \n","    Y_test_ = Y[:round(np.size(X,0)*0.2)]\n","    C_test = C[:round(np.size(X,0)*0.2)]\n","    R_test = R[:round(np.size(X,0)*0.2)]\n","    RR_test = RR[:round(np.size(X,0)*0.2)]\n","\n","    X_valid_ = X[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)] \n","    Y_valid_ = Y[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)]\n","    C_valid = C[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)]\n","    R_valid = R[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)]\n","    RR_valid = RR[round(np.size(X,0)*0.2):round(np.size(X,0)*0.3)]\n","\n","    X_train = X[round(np.size(X,0)*0.3):]\n","    Y_train = Y[round(np.size(X,0)*0.3):]\n","    RR_train = RR[round(np.size(X,0)*0.3):]\n","  # C_train = C[:round(np.size(X,0)*dataset_split)]\n","    R_train = R[round(np.size(X,0)*0.3):]\n","  # P_train = P[:round(np.size(X,0)*dataset_split)]\n","\n","  elif split == 3:\n","    print(\"Split 3\")\n","    X_test_ = X[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)] \n","    Y_test_ = Y[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)]\n","    C_test = C[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)]\n","    R_test = R[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)]\n","    RR_test = RR[round(np.size(X,0)*0.2):round(np.size(X,0)*0.4)]\n","\n","    X_valid_ = X[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)] \n","    Y_valid_ = Y[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)]\n","    C_valid = C[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)]\n","    R_valid = R[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)]\n","    RR_valid = RR[round(np.size(X,0)*0.4):round(np.size(X,0)*0.5)]\n","\n","    X_train = X[:round(np.size(X,0)*0.2)]\n","    Y_train = Y[:round(np.size(X,0)*0.2)]\n","    RR_train = RR[:round(np.size(X,0)*0.2)]\n","    R_train = R[:round(np.size(X,0)*0.2)]\n","\n","    X_train = np.append(X_train, X[round(np.size(X,0)*0.5):], axis = 0)\n","    Y_train = np.append(Y_train, Y[round(np.size(X,0)*0.5):], axis = 0)\n","    RR_train = np.append(RR_train, RR[round(np.size(X,0)*0.5):], axis = 0)\n","    R_train = np.append(R_train, R[round(np.size(X,0)*0.5):], axis = 0)\n","\n","  elif split == 4:\n","    print(\"Split 4\")\n","    X_test_ = X[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)] \n","    Y_test_ = Y[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)]\n","    C_test = C[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)]\n","    R_test = R[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)]\n","    RR_test = RR[round(np.size(X,0)*0.4):round(np.size(X,0)*0.6)]\n","\n","    X_valid_ = X[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)] \n","    Y_valid_ = Y[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)]\n","    C_valid = C[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)]\n","    R_valid = R[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)]\n","    RR_valid = RR[round(np.size(X,0)*0.6):round(np.size(X,0)*0.7)]\n","\n","    X_train = X[:round(np.size(X,0)*0.4)] \n","    Y_train = Y[:round(np.size(X,0)*0.4)]\n","    RR_train = RR[:round(np.size(X,0)*0.4)]\n","    R_train = R[:round(np.size(X,0)*0.4)]\n","\n","    X_train = np.append(X_train, X[round(np.size(X,0)*0.7):], axis = 0)\n","    Y_train = np.append(Y_train, Y[round(np.size(X,0)*0.7):], axis = 0)\n","    RR_train = np.append(RR_train, RR[round(np.size(X,0)*0.7):], axis = 0)\n","    R_train = np.append(R_train, R[round(np.size(X,0)*0.7):], axis = 0)\n","\n","  elif split == 5:\n","    print(\"Split 5\")\n","    X_test_ = X[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)] \n","    Y_test_ = Y[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)]\n","    C_test = C[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)]\n","    R_test = R[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)]\n","    RR_test = RR[round(np.size(X,0)*0.6):round(np.size(X,0)*0.8)]\n","\n","    X_valid_ = X[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)] \n","    Y_valid_ = Y[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)]\n","    C_valid = C[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)]\n","    R_valid = R[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)]\n","    RR_valid = RR[round(np.size(X,0)*0.8):round(np.size(X,0)*0.9)]\n","\n","    X_train = X[:round(np.size(X,0)*0.6)]\n","    Y_train = Y[:round(np.size(X,0)*0.6)]\n","    RR_train = RR[:round(np.size(X,0)*0.6)]\n","    R_train = R[:round(np.size(X,0)*0.6)]\n","\n","    X_train = np.append(X_train, X[round(np.size(X,0)*0.9):], axis = 0)\n","    Y_train = np.append(Y_train, Y[round(np.size(X,0)*0.9):], axis = 0)\n","    RR_train = np.append(RR_train, RR[round(np.size(X,0)*0.9):], axis = 0)\n","    R_train = np.append(R_train, R[round(np.size(X,0)*0.9):], axis = 0)\n","\n","  X_valid = X_valid_[C_valid]\n","  Y_valid = Y_valid_[C_valid]\n","  RR_valid = RR_valid[C_valid]\n","  X_test = X_test_[C_test]\n","  Y_test = Y_test_[C_test]\n","  RR_test = RR_test[C_test]\n","  R_valid = R_valid[C_valid]\n","  R_test = R_test[C_test]\n","\n","  print(\"Validation set\",X_valid.shape)\n","  print(\"Test set\",X_test.shape)\n","  if normalization:\n","  ############# old normalization matteo #################\n","  #  for i in range(np.size(X_train,0)):\n","  #      X_train[i]=X_train[i]/np.max(np.absolute(X_train[i]))\n","  #  for i in range(np.size(X_valid,0)):\n","  #      X_valid[i]=X_valid[i]/np.max(np.absolute(X_valid[i]))\n","  #  for i in range(np.size(X_test,0)):\n","  #      X_test[i]=X_test[i]/np.max(np.absolute(X_test[i])) \n","\n","  ############# normalization paola #################\n","   # max_training = np.max(np.absolute(X_train))\n","    #for i in range(np.size(X_train,0)):\n","   #     X_train[i]=X_train[i]/max_training\n","   # for i in range(np.size(X_valid,0)):\n","     #   X_valid[i]=X_valid[i]/max_training\n","   # for i in range(np.size(X_test,0)):\n","     #   X_test[i]=X_test[i]/max_training\n","\n","############# normalization RR #################\n","    for i in range(np.size(RR_train,0)):\n","        RR_train[i][0]=RR_train[i][0]/np.mean(RR_train)\n","        RR_train[i][1]=RR_train[i][1]/np.mean(RR_train)\n","    for i in range(np.size(RR_valid,0)):\n","        RR_valid[i][0]=RR_valid[i][0]/np.mean(RR_valid)\n","        RR_valid[i][1]=RR_valid[i][1]/np.mean(RR_valid)\n","    for i in range(np.size(RR_test,0)):\n","        RR_test[i][0]=RR_test[i][0]/np.mean(RR_test)\n","        RR_test[i][1]=RR_test[i][1]/np.mean(RR_test)\n","\n","  X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1,1)\n","  X_valid = X_valid.reshape(X_valid.shape[0],X_valid.shape[1],1,1)\n","  X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1,1)\n","\n","  print(\"X train_shape\",X_train.shape)\n","  print(\"X valid_shape\",X_valid.shape)\n","  print(\"Y train shape\", Y_train.shape)\n","  print(\"Y valid shape\", Y_valid.shape)\n","  print(\"X_test shape\", X_test.shape)\n","  print(\"Y test shape\", Y_test.shape)\n","\n","  class_types = labels\n","  f_log.close()\n","\n","  return X_train, Y_train, X_valid, Y_valid, X_test, Y_test, R_train, R_valid, R_test, RR_train, RR_valid, RR_test, class_types, session_path"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1684271529174,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"a57RXTivtgfB"},"outputs":[],"source":["session_name = 'train_session'\n","kernel = 3\n","emb_depth = 1\n","embedding = 16\n","transformer_layers = 1\n","hidden_size = 2 # key dim in attention layer\n","num_heads =  8\n","mlp_dim = 128\n","overwrite = 1\n","random_seed=0\n","batch_size=128\n","half_window = 99\n","use_cls_token = 0\n","split = 1\n","cat_RR = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2073727,"status":"ok","timestamp":1684273602887,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"-l9WLt6jtC6S","outputId":"ce4860e7-d9dc-47f7-e3b6-b30758a3ad54"},"outputs":[],"source":["#############################################################################################################\n","#Create session\n","\n","if IN_COLAB:\n","    session_path = f\"/content/gdrive/MyDrive/output/train/em_3_10_24/\"\n","else:\n","    session_path = f\"output/train/em_3_10_24/\"\n","\n","os.makedirs(session_path, exist_ok=True)\n"," \n","#############################################################################################################\n","\n","#############################################################################################################\n","#Load dataset ECG\n","\n","# x_train, y_train, x_valid, y_valid, x_test, y_test, R_train, R_valid, R_test, RR_train, RR_valid, RR_test, class_types, session_path = load_ECG_MIT_BIH_mixedtestset_7_1_2(half_window, session_name, overwrite,1,split,IN_COLAB)\n","\n","if IN_COLAB:\n","    dataset_path = \"/content/gdrive/MyDrive/dataset/mit-bih-arrhythmia-database-1.0.0/\"\n","    other_dataset_path = [\"/content/gdrive/MyDrive/dataset/mit-bih-arrhythmia-database-1.0.0_em3/\",\n","                          \"/content/gdrive/MyDrive/dataset/mit-bih-arrhythmia-database-1.0.0_em10/\",\n","                          \"/content/gdrive/MyDrive/dataset/mit-bih-arrhythmia-database-1.0.0_em24/\",\n","                          ]\n","else:\n","    dataset_path = \"../../dataset/mit-bih-arrhythmia-database-1.0.0/\"\n","    other_dataset_path = [\"../../dataset/mit-bih-arrhythmia-database-1.0.0_em3/\",\n","                          \"../../dataset/mit-bih-arrhythmia-database-1.0.0_em10/\",\n","                          \"../../dataset/mit-bih-arrhythmia-database-1.0.0_em24/\",\n","                          ]\n","\n","if dataset_path.endswith(\"mit-bih-arrhythmia-database-1.0.0/\"):\n","    iswfdbdataset = True\n","else:\n","    iswfdbdataset = False\n","\n","x_train, y_train, x_valid, y_valid, x_test, y_test,  R_train, R_valid,R_test, RR_train, RR_valid, RR_test, class_types = load_ECG_MIT_BIH_mixedtestset_7_1_2_mod(half_window, overwrite, 1, split, iswfdbdataset, dataset_path) # , session_path\n"," \n","for o_dataset_path in other_dataset_path:\n","    if o_dataset_path.endswith(\"mit-bih-arrhythmia-database-1.0.0/\"):\n","        iswfdbdataset = True\n","    else:\n","        iswfdbdataset = False\n","    o_x_train, o_y_train, o_x_valid, o_y_valid, o_x_test, o_y_test,  o_R_train, o_R_valid, o_R_test, o_RR_train, o_RR_valid, o_RR_test, o_class_types = load_ECG_MIT_BIH_mixedtestset_7_1_2_mod(half_window, overwrite, 1, split, iswfdbdataset, o_dataset_path) # , session_path\n","    x_train = np.concatenate((x_train, o_x_train), axis=0)\n","    y_train = np.concatenate((y_train, o_y_train), axis=0)\n","    x_valid = np.concatenate((x_valid, o_x_valid), axis=0)\n","    y_valid = np.concatenate((y_valid, o_y_valid), axis=0)\n","    x_test = np.concatenate((x_test, o_x_test), axis=0)\n","    y_test = np.concatenate((y_test, o_y_test), axis=0)\n","    R_train = np.concatenate((R_train, o_R_train), axis=0)\n","    R_valid = np.concatenate((R_valid, o_R_valid), axis=0)\n","    R_test = np.concatenate((R_test, o_R_test), axis=0)\n","    RR_train = np.concatenate((RR_train, o_RR_train), axis=0)\n","    RR_valid = np.concatenate((RR_valid, o_RR_valid), axis=0)\n","    RR_test = np.concatenate((RR_test, o_RR_test), axis=0)\n","\n","print(\"Final x_train shape:\", x_train.shape)\n","print(\"Final y_train shape:\", y_train.shape)\n","print(\"Final x_valid shape:\", x_valid.shape)\n","print(\"Final y_valid shape:\", y_valid.shape)\n","print(\"Final x_test shape:\", x_test.shape)\n","print(\"Final y_test shape:\", y_test.shape)\n","print(\"Final R_train shape:\", R_train.shape)\n","print(\"Final R_valid shape:\", R_valid.shape)\n","print(\"Final R_test shape:\", R_test.shape)\n","print(\"Final RR_train shape:\", RR_train.shape)\n","print(\"Final RR_valid shape:\", RR_valid.shape)\n","print(\"Final RR_test shape:\", RR_test.shape)\n","\n","classes=np.unique(y_train)\n","print(classes)\n","print(session_path)\n","print ('check shapes: ', x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n","\n","train_lab_categorical = tf.keras.utils.to_categorical(y_train, num_classes=len(np.unique(y_train)))\n","valid_lab_categorical = tf.keras.utils.to_categorical(y_valid, num_classes=len(np.unique(y_train)))\n","test_lab_categorical = tf.keras.utils.to_categorical(y_test, num_classes=len(np.unique(y_train)))\n","\n","print(\"Check composition of datasets\")\n","print(\"Train\",np.sum(train_lab_categorical.swapaxes(1,0)[0]), np.sum(train_lab_categorical.swapaxes(1,0)[1]), np.sum(train_lab_categorical.swapaxes(1,0)[2]), np.sum(train_lab_categorical.swapaxes(1,0)[3]), np.sum(train_lab_categorical.swapaxes(1,0)[4]))\n","print(\"Valid\",np.sum(valid_lab_categorical.swapaxes(1,0)[0]), np.sum(valid_lab_categorical.swapaxes(1,0)[1]), np.sum(valid_lab_categorical.swapaxes(1,0)[2]), np.sum(valid_lab_categorical.swapaxes(1,0)[3]), np.sum(valid_lab_categorical.swapaxes(1,0)[4]))\n","print(\"Test\",np.sum(test_lab_categorical.swapaxes(1,0)[0]), np.sum(test_lab_categorical.swapaxes(1,0)[1]), np.sum(test_lab_categorical.swapaxes(1,0)[2]), np.sum(test_lab_categorical.swapaxes(1,0)[3]), np.sum(test_lab_categorical.swapaxes(1,0)[4]))\n","\n","training_data = tf.data.Dataset.from_tensor_slices(x_train)\n","training_RR = tf.data.Dataset.from_tensor_slices(RR_train)\n","train_labels = tf.data.Dataset.from_tensor_slices(train_lab_categorical)\n","validation_data = tf.data.Dataset.from_tensor_slices(x_valid)\n","valid_RR = tf.data.Dataset.from_tensor_slices(RR_valid)\n","valid_labels = tf.data.Dataset.from_tensor_slices(valid_lab_categorical)\n","test_data = tf.data.Dataset.from_tensor_slices(x_test)\n","test_RR = tf.data.Dataset.from_tensor_slices(RR_test)\n","test_labels = tf.data.Dataset.from_tensor_slices(test_lab_categorical)\n","\n","autotune = tf.data.AUTOTUNE \n","\n","X_t = tf.data.Dataset.zip((training_data, training_RR)).map(lambda x1, x2: {'x1': x1, 'x2': x2}) #map two different inputs, x2 is the RR features\n","X_v = tf.data.Dataset.zip((validation_data, valid_RR)).map(lambda x1, x2: {'x1': x1, 'x2': x2})\n","X_T = tf.data.Dataset.zip((test_data, test_RR)).map(lambda x1, x2: {'x1': x1, 'x2': x2})\n","\n","\n","XY_t = tf.data.Dataset.zip((X_t, train_labels))\n","XY_v = tf.data.Dataset.zip((X_v, valid_labels))\n","XY_T = tf.data.Dataset.zip((X_T, test_labels))\n","\n","train_ds = (XY_t.batch(batch_size).map(lambda x, y: (x, y), num_parallel_calls=autotune).prefetch(autotune))\n","valid_ds = (XY_v.batch(batch_size).map(lambda x, y: (x, y), num_parallel_calls=autotune).prefetch(autotune))\n","test_ds = (XY_T.batch(batch_size).map(lambda x, y: (x, y), num_parallel_calls=autotune).prefetch(autotune))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40,"status":"ok","timestamp":1684273602888,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"qncPEAlmtine","outputId":"49e75dad-973c-45f9-d952-3dfcb4ef8215"},"outputs":[],"source":["\n","#############################################################################################################\n","#Model definition\n","\n","def generate_patch_conv_orgPaper_f(embedding, inputs):\n","  print(inputs.shape) \n","  patches = layers.Conv2D(filters=embedding, kernel_size=[kernel,1], strides=[kernel,1], padding='valid',input_shape=inputs.shape[1:])(inputs) \n","  for i in range(emb_depth-1):\n","    patches = layers.BatchNormalization()(patches) \n","    patches = layers.Conv2D(filters=embedding, kernel_size=[kernel,1], strides=[kernel,1], padding='valid',input_shape=patches.shape)(patches)  \n","  row_axis, col_axis = (1, 2) # channels last images\n","  print(patches.shape)\n","  seq_len = (patches.shape[row_axis]) * (patches.shape[col_axis])\n","  print(seq_len)\n","  x = tf.reshape(patches, [-1, seq_len, embedding])\n","  print(x.shape)\n","  return x\n","\n","\n","class AddPositionEmbs(layers.Layer):\n","  \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n","\n","  def __init__(self, posemb_init=None, **kwargs):\n","    super().__init__(**kwargs)\n","    self.posemb_init = posemb_init\n","    #self.posemb_init=[tf.keras.initializers.RandomNormal(stddev=0.02), name='posembed_input'] # used in original code\n","\n","  def get_config(self):\n","        config = super().get_config().copy()\n","        config.update({\n","            'posembed_input': self.posemb_init\n","        })\n","        return config\n","\n","  def build(self, inputs_shape):\n","    pos_emb_shape = (1, inputs_shape[1], inputs_shape[2])\n","    self.pos_embedding = self.add_weight('pos_embedding', pos_emb_shape, initializer=self.posemb_init)\n","\n","  def call(self, inputs, inputs_positions=None):\n","    # inputs.shape is (batch_size, seq_len, emb_dim).\n","    pos_embedding = tf.cast(self.pos_embedding, inputs.dtype)\n","\n","    return inputs + pos_embedding\n","\n","\n","class CLS_Token(layers.Layer):\n","  \"\"\"Adds (optionally learned) positional embeddings to the inputs.\"\"\"\n","\n","  def __init__(self, token_init=None, **kwargs):\n","    super().__init__(**kwargs)\n","    self.token_init = token_init\n","\n","\n","  def get_config(self):\n","        config = super().get_config().copy()\n","        config.update({\n","            'token_input': self.token_init\n","        })\n","        return config\n","\n","  def build(self, inputs_shape):\n","    token_shape = (1, 1, inputs_shape[2])\n","    print(token_shape)\n","    self.token = self.add_weight('token', token_shape, initializer=self.token_init,trainable=True)\n","    super(CLS_Token, self).build(inputs_shape)\n","\n","  def call(self, inputs, inputs_positions=None):\n","    cls_token = tf.cast(self.token, inputs.dtype)\n","    cls_token_new = repeat_const(inputs, cls_token) \n","    return tf.concat((cls_token_new, inputs), axis=1)\n","\n","\n","def mlp_block_f(mlp_dim, inputs):\n","  x = layers.Dense(units=mlp_dim, activation=tf.nn.gelu)(inputs)\n","  x = layers.Dropout(rate=0.1)(x) # dropout rate is from original paper,\n","  x = layers.Dense(units=inputs.shape[-1], activation=tf.nn.gelu)(x)\n","  x = layers.Dropout(rate=0.1)(x)\n","  return x\n","\n","\n","def Encoder1Dblock_f_postNorm(num_heads, mlp_dim, inputs):\n"," # x = layers.LayerNormalization(dtype=inputs.dtype)(inputs) # pre-Norm\n"," # x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_size, dropout=0.1)(x, x) # pre-Norm #self attention multi-head, dropout_rate is from original implementation\n"," # x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1], dropout=0.1)(x, x) # self attention multi-head, dropout_rate is from original implementation\n","  x = inputs # post-Norm\n","  x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_size, dropout=0.1)(x, x) # post-Norm #self attention multi-head, dropout_rate is from original implementation\n","  x = layers.LayerNormalization(dtype=x.dtype)(x) # post-Norm\n","  x = layers.Add()([x, inputs]) # 1st residual part \n"," # y = layers.LayerNormalization(dtype=x.dtype)(x) # pre-Norm\n"," # y = mlp_block_f(mlp_dim, y) #pre-Norm\n","  y = mlp_block_f(mlp_dim, x) #post-Norm\n","  y = layers.LayerNormalization(dtype=y.dtype)(y) # post-Norm\n","  y_1 = layers.Add()([y, x]) #2nd residual part \n","  return y_1\n","\n","\n","def Encoder1Dblock_f(num_heads, mlp_dim, inputs):\n","  x = layers.LayerNormalization(dtype=inputs.dtype)(inputs)\n"," # x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputs.shape[-1], dropout=0.1)(x, x) # self attention multi-head, dropout_rate is from original implementation\n","  x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=hidden_size, dropout=0.1)(x, x) # self attention multi-head, dropout_rate is from original implementation\n","  x = layers.Add()([x, inputs]) # 1st residual part \n","  y = layers.LayerNormalization(dtype=x.dtype)(x)\n","  y = mlp_block_f(mlp_dim, y)\n","  y_1 = layers.Add()([y, x]) #2nd residual part \n","  return y_1\n","\n","\n","\n","def Encoder_f(num_layers, mlp_dim, num_heads, inputs):\n","  x = AddPositionEmbs(posemb_init=tf.keras.initializers.RandomNormal(stddev=0.02), name='posembed_input')(inputs)\n","  x = layers.Dropout(rate=0.2)(x)\n","  for _ in range(num_layers):\n","   # x = Encoder1Dblock_f(num_heads, mlp_dim, x)\n","    x = Encoder1Dblock_f_postNorm(num_heads, mlp_dim, x)\n","\n","  encoded = layers.LayerNormalization(name='encoder_norm')(x)\n","  return encoded\n","\n","######################################\n","\n","def repeat_const(tensor, myconst):\n","    shapes = tf.shape(tensor)\n","    return tf.repeat(myconst, shapes[0], axis=0)\n","\n","def read_cls_token(tensor, myconst):\n","    #temp = tensor[1:]\n","    print(tensor.shape)\n","    myconst=tensor[:,0]\n","    return myconst\n","\n","def build_ViT(use_cls_token, cat_RR):\n","  inputs = layers.Input(shape=x_train.shape[1:],name='x1')\n","  if cat_RR:\n","    RR_feat = layers.Input(shape=RR_train.shape[1:],name='x2')\n","    print(\"shape of RR feat\",RR_train.shape[1:])\n","    print(type(RR_feat))\n","  patches = generate_patch_conv_orgPaper_f(embedding, inputs)\n","  print(type(patches))\n","  if use_cls_token: \n","    patches_new = CLS_Token(token_init=tf.keras.initializers.zeros(), name='token_input')(patches)   \n","  else:\n","    patches_new = patches\n","  encoder_out = Encoder_f(transformer_layers, mlp_dim, num_heads, patches_new)  \n","  if use_cls_token:\n","    print(encoder_out.shape)\n","    im_representation = encoder_out[:,0]\n","  else:\n","    im_representation = tf.reduce_mean(encoder_out, axis=1)\n","  if cat_RR:\n","    im_representation = tf.concat([im_representation,RR_feat],axis=-1)  \n","\n","  logits = layers.Dense(units=len(class_types), name='head', kernel_initializer=tf.keras.initializers.zeros())(im_representation) \n","  print(logits.shape)\n","\n","  final_model = tf.keras.Model(inputs = [inputs,RR_feat], outputs = logits)\n","  return final_model\n","\n","ViT_model = build_ViT(use_cls_token,cat_RR)\n","ViT_model.summary()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1684273602888,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"cbze8G_Qtm17"},"outputs":[],"source":["\n","import keras.backend as K\n","class CustomCallback(keras.callbacks.Callback):\n","\n","   def on_train_begin(self, logs=None): # this runs on the beginning of training\n","        f = open(session_path + \"lr_log.txt\",\"r\")\n","        if os.path.getsize(session_path + \"lr_log.txt\") > 0:\n","             print(\"Found file \")\n","             for line in f:\n","               pass\n","             last_line = line\n","             print(last_line)\n","             words = line.split()\n","             print(words)\n","             last_lr = float(words[-1])\n","             print(\"last_lr\",last_lr)\n","             tf.keras.backend.set_value(self.model.optimizer.lr, float(last_lr)) # set the learning rate in the optimizer\n","        f.close()\n","        #tf.keras.backend.set_value(self.model.optimizer.lr, 5e-6) # set the learning rate in the optimizer\n","        print(\"after log file read\")\n","        lr=float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate to insure it is set\n","        print('Optimizer learning rate set to ', lr)\n","\n","   def on_epoch_begin(self, epoch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n","        lr = K.eval(self.model.optimizer.lr.numpy())\n","        print(\"\\nLearning rate at epoch {} is {}\".format(epoch, lr))        \n","        f = open(session_path + \"lr_log.txt\",\"a\")\n","        f.write(\"\\nLearning rate at epoch {} is {}\".format(epoch, lr))\n","        f.close()\n","\n","\n","   def on_epoch_end(self, epoch, logs=None):\n","        keys = list(logs.keys())\n","        print(\"End epoch {} of training; train_loss: {:1.4f}, train_accuracy: {:2.4f}, validation_loss: {:1.4f}, validation_accuracy: {:2.4f}\\n\".format(epoch, logs[\"loss\"], logs[\"accuracy\"], logs[\"val_loss\"], logs[\"val_accuracy\"]))\n","        f = open(session_path + \"train_log.txt\",\"a\")\n","        f.write(\"End epoch {} of training; train_loss: {:1.4f}, train_accuracy: {:2.4f}, validation_loss: {:1.4f}, validation_accuracy: {:2.4f}\\n\".format(epoch, logs[\"loss\"], logs[\"accuracy\"], logs[\"val_loss\"], logs[\"val_accuracy\"]))\n","        f.close()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1684273602889,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"xFHxA6j8t1aY"},"outputs":[],"source":["\n","def print_stats(predictions, labels):\n","  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n","  print(\"Precision = {}\".format(precision_score(labels, predictions,average=\"micro\")))\n","  print(\"Recall = {}\".format(recall_score(labels, predictions,average=\"micro\")))\n","\n","def conf_matrix_ecg(predictions,y_test,class_types, session_path): \n","    ''' Plots conf. matrix and classification report '''\n","    print(class_types, len(class_types), y_test.shape)\n","    cm=confusion_matrix(y_test, np.argmax(np.round(predictions), axis=1),labels= np.arange(len(class_types)))\n","    print(cm)\n","    print_stats(np.argmax(np.round(predictions), axis=1), y_test)\n","    plt.figure(figsize=(12,12))\n","    sns_hmp = sns.heatmap(cm, annot=True, xticklabels = [class_types[i] for i in range(len(class_types))], \n","                yticklabels = [class_types[i] for i in range(len(class_types))], fmt=\"d\")\n","    fig = sns_hmp.get_figure()\n","    fig.savefig(session_path+'confusion_matrix.png', dpi=250)\n","    cr=classification_report(y_test,\n","                                np.argmax(np.round(predictions), axis=1),labels= np.arange(len(class_types)), \n","                                target_names=[class_types[i] for i in range(len(class_types))])\n","    print(cr)\n","    print(\"\\n\\nMatteo metrics\\n\")\n","    print(\"\\n\\nTN FP FN TP\\n\")\n","    n_rows = len(cm)\n","    TN = cm[0][0]\n","    FP = 0\n","    FN = 0\n","    TP = 0\n","    for i in range(n_rows):\n","      if i > 0:\n","        TP = TP + cm[i][i]\n","        FN = FN + cm[i][0]\n","        FP = FP + cm[0][i] \n","        if i != 1:\n","          FP = FP + cm[1][i]\n","        if i != 2:\n","          FP = FP + cm[2][i]\n","        if i != 3:\n","          FP = FP + cm[3][i]\n","    print(TN, FP, FN, TP)\n","    accuracy, sensitivity, specificity, precision, f1_score = calculate_metrics_from_cm(TN, FP, FN, TP)\n","    print(\"Accuracy Sensitivity Specificity Precision F1\\n\")\n","    print(str(accuracy) + \" \" + str(sensitivity) + \" \" + str(specificity) + \" \" + str(precision) + \" \" + str(f1_score)+\"\\n\")\n","    return accuracy\n","\n","def calculate_metrics_from_cm(TN, FP, FN, TP):\n","    if(TP + FN != 0):\n","        sensitivity = (TP / (TP + FN)) * 100\n","    else:\n","        sensitivity = 0\n","    if(TN + FP != 0):\n","        specificity = (TN / (TN + FP)) * 100\n","    else:\n","        specificity = 0\n","    accuracy = (TP + TN) / (TN + FN + TP + FP) * 100\n","    if(TP + FP != 0):    \n","        precision = (TP / (TP + FP)) * 100\n","    else:\n","        precision = 0\n","    print(str(precision>0))\n","    print(str(sensitivity>0.0))\n","    if precision > 0 or sensitivity > 0.0:\n","        f1_score = (2 * ((precision*sensitivity)/(precision+sensitivity)))\n","    else:    \n","        f1_score = 0 \n","    return accuracy, sensitivity, specificity, precision, f1_score\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1684273602889,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"Cjj-xEnHt17q","outputId":"e7e27620-7fbb-4970-ccb1-e6ba1615fa21"},"outputs":[],"source":["filepath = session_path+ \"ecg_transformer_acc_\"\n","filepath_loss = session_path+ \"ecg_transformer_loss_\"\n","filepath_curr = session_path+ \"ecg_transformer_curr.h5\"\n","\n","if not os.path.exists(session_path + \"train_log.txt\"):\n","    f = open(session_path + \"train_log.txt\",\"w\")\n","    f.close()\n","    f = open(session_path + \"lr_log.txt\",\"w\")\n","    f.close()\n","    epoch_done = 0\n","else:\n","    model = session_path + \"ecg_transformer_curr.h5\"\n","    ViT_model.load_weights(model)\n","    print('model exist')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3049706,"status":"ok","timestamp":1684276652577,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"Lh4MrmL5yv-H","outputId":"002f5c8f-13e0-4c95-d85c-6496a99df496"},"outputs":[],"source":["ViT_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-3), \n","                  loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n","                  metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"accuracy\"), tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top5 acc')]) \n","\n","#queste no :)\n","#old_weights = ViT_model.get_weights()\n","#print(len(old_weights))\n","#print(range(len(old_weights)))\n","#for i in range(len(old_weights))[:-2]:  #load weights of the model pretrained on AFIB dataset\n","#  old_weights[i] = np.load(\"output/train_raw/model6_7_1_2_atrialpretr/\" + str(i) + \"AFIB_weights.npy\")\n","#  print(str(i) + \"AFIB_weights.npy\")\n","#ViT_model.set_weights(old_weights)\n","#ViT_model.save(session_path + \"start_model.h5\")\n","\n","checkpoint = ModelCheckpoint(filepath + \"{epoch:02d}-{val_accuracy:.4f}.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n","checkpoint_loss = ModelCheckpoint(filepath_loss + \"{epoch:02d}-{val_loss:.4f}.h5\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","checkpoint_curr = ModelCheckpoint(filepath_curr, monitor='val_loss', verbose=1, save_best_only=False, mode='min')\n","\n","writelog = CustomCallback()\n","print(\"starting checkpoint\",checkpoint.best)\n","\n","f = open(session_path + \"train_log.txt\",\"r\")\n","if os.path.getsize(session_path + \"train_log.txt\") > 0:\n","    print(\"Found file \")\n","    i = 0\n","    for n, line in enumerate(f.readlines()):\n","            split1, split2, split3, split4 = line.split(\",\")\n","            epoch_done = n + 1 # int(re.search(r'\\d+', split1[10:15]).group()) + 1\n","            train_loss = float(split1[-5:])\n","            validation_loss = float(split3[-5:])\n","            train_accuracy = float(split2[-6:])\n","            validation_accuracy = float(split4[-6:])\n","            if i == 0:\n","               best = validation_accuracy\n","               best_loss = validation_loss\n","               i = i + 1\n","            elif validation_accuracy > best:\n","               best = validation_accuracy\n","            if validation_loss < best_loss:\n","               best_loss = validation_loss\n","\n","    checkpoint.best = best\n","    checkpoint_loss.best = best_loss\n","    print(\"new checkpoint is \",checkpoint.best, \" and \", checkpoint_loss.best)\n","\n","#results = ViT_model.evaluate(valid_ds, batch_size=16)\n","#print(results, ViT_model.metrics_names)\n","#checkpoint.best = results[1]\n","#checkpoint_loss.best = results[0]\n","\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8,\n","                              patience=5, min_lr=1e-7, verbose=1)\n","\n","print(train_ds)\n","print(valid_ds)\n","print(y_train.shape)\n","class_weights = class_weight.compute_class_weight('balanced',\n","                                                 classes=np.unique(y_train),\n","                                                 y=y_train)\n","\n","class_weights = dict(zip(np.unique(y_train), class_weights))\n","ViT_Train = ViT_model.fit(train_ds, \n","                       epochs = 300 - epoch_done, #aggiorna il numero di epoche residue se si interrompe il runtime\n","                       validation_data=valid_ds, callbacks=[reduce_lr, checkpoint, checkpoint_loss, checkpoint_curr, writelog])#,class_weight=class_weights)\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1684276652577,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"znuvHk3TzBUI"},"outputs":[],"source":["def get_file_with_highest_accuracy(folder_path):\n","    pattern = re.compile(r'ecg_transformer_acc_(\\d+)-(\\d+\\.\\d+)')\n","    max_accuracy = 0\n","    max_accuracy_file = None\n","\n","    for file in os.listdir(folder_path):\n","        match = pattern.match(file)\n","        if match:\n","            accuracy = float(match.group(2))\n","            if accuracy > max_accuracy:\n","                max_accuracy = accuracy\n","                max_accuracy_file = file\n","\n","    return max_accuracy_file\n","\n","file_with_highest_accuracy = get_file_with_highest_accuracy(session_path)"]},{"cell_type":"markdown","metadata":{"id":"NrnH_hmL1cZp"},"source":["**Test**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":21362,"status":"ok","timestamp":1684276674297,"user":{"displayName":"Nuovo Eolab","userId":"06990839351255227506"},"user_tz":-120},"id":"Bmv6ULCh1gJc","outputId":"9ff9153c-06ef-41a6-9b07-796bf4455f76"},"outputs":[],"source":["filepath = session_path + file_with_highest_accuracy #f\"ecg_transformer_acc_04-0.9621.h5\" #il test va aggiornato di volta in volta con il nome del modello da testare\n","ViT_model.load_weights(filepath)\n","\n","\n","#load best model\n","ViT_model.load_weights(filepath)\n","\n","\n","pred_class_eeg = ViT_model.predict(test_ds, batch_size=1)\n","\n","res_log = open(\"res_log_keras_out.txt\",\"w\")\n","print(pred_class_eeg.shape)\n","\n","for i in range(pred_class_eeg.shape[0]):\n","  res_log.write(\"\\nout: \" + str(pred_class_eeg[i]) + \" class: \" + str(np.argmax(np.round(pred_class_eeg[i]))))    \n","#  res_log.write(\"\\nclass: \" + str(np.argmax(np.round(pred_class_eeg[i])))) \n"," # print(i)   \n","  if np.argmax(np.round(pred_class_eeg[i])) != y_test[i]:\n","   print(\"predicted: \",np.argmax(np.round(pred_class_eeg[i])), \" truth: \",y_test[i], \" index: \",i)\n","\n","conf_matrix_ecg(pred_class_eeg, y_test,class_types,session_path)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
